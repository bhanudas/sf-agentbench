# SF-AgentBench Configuration
# See docs/development/Salesforce AI Benchmark Design.md for full documentation

# Directory paths
tasks_dir: tasks
results_dir: results
logs_dir: logs

# Agent configuration
agent:
  id: claude-code              # Agent identifier for results tracking
  type: claude                 # Agent type: claude, openai, gemini, custom
  model: claude-sonnet-4-20250514  # Model to use for the agent
  api_key_env: ANTHROPIC_API_KEY  # Environment variable containing API key
  max_iterations: 50           # Maximum agent iterations per task
  timeout_seconds: 1800        # Timeout per task (30 minutes)

# Salesforce CLI configuration
sf_cli_path: sf
devhub_username: null  # Set to your DevHub username

# Scratch Org settings
scratch_org:
  default_duration_days: 1
  edition: Developer
  wait_minutes: 10
  use_snapshots: false
  snapshot_name: null

# Evaluation weights (must sum to 1.0)
evaluation_weights:
  deployment: 0.20        # Layer 1: Can it deploy?
  functional_tests: 0.40  # Layer 2: Do tests pass?
  static_analysis: 0.10   # Layer 3: Code quality (PMD)
  metadata_diff: 0.15     # Layer 4: Configuration accuracy
  rubric: 0.15            # Layer 5: LLM-as-a-Judge

# PMD/Code Analyzer settings
pmd:
  enabled: true
  ruleset: default
  critical_weight: 3.0
  high_weight: 2.0
  medium_weight: 1.0
  low_weight: 0.5
  max_penalty: 0.10

# LLM-as-a-Judge rubric settings
rubric:
  enabled: true
  model: claude-3-5-sonnet-20241022
  temperature: 0.0
  max_tokens: 2048
  num_evaluations: 1

# Execution settings
parallel_runs: 1
timeout_minutes: 60
cleanup_orgs: true

# Logging
verbose: false
log_level: INFO
