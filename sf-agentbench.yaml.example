# SF-AgentBench Configuration
# See docs/development/Salesforce AI Benchmark Design.md for full documentation
# Run `sf-agentbench list-models` to see all supported models

# Directory paths
tasks_dir: tasks
results_dir: results
logs_dir: logs

# Agent configuration
# Supported models include:
#   Anthropic: claude-sonnet-4-20250514, claude-3-5-sonnet-20241022, claude-3-opus-20240229
#   OpenAI:    gpt-4o, gpt-4o-mini, gpt-4-turbo, o1, o1-mini, o3-mini
#   Google:    gemini-2.0-flash, gemini-2.0-flash-thinking, gemini-1.5-pro, gemini-1.5-flash
agent:
  id: my-agent                 # Agent identifier for results tracking
  type: claude                 # Agent type: claude, openai, gemini, custom
  model: claude-sonnet-4-20250514  # Model to use (run `sf-agentbench list-models` for options)
  # api_key_env: ANTHROPIC_API_KEY  # Auto-detected from model, or override here
  max_iterations: 50           # Maximum agent iterations per task
  timeout_seconds: 1800        # Timeout per task (30 minutes)

# Custom models (add your own models here)
# custom_models:
#   - id: my-custom-llm
#     name: My Custom LLM
#     provider: custom         # anthropic, openai, google, or custom
#     api_key_env: MY_API_KEY
#     context_window: 128000

# Salesforce CLI configuration
sf_cli_path: sf
devhub_username: null  # Set to your DevHub username or alias (e.g., "my-devhub")

# Scratch Org settings
scratch_org:
  default_duration_days: 1
  edition: Developer
  wait_minutes: 10
  use_snapshots: false
  snapshot_name: null

# Evaluation weights (must sum to 1.0)
evaluation_weights:
  deployment: 0.20        # Layer 1: Can it deploy?
  functional_tests: 0.40  # Layer 2: Do tests pass?
  static_analysis: 0.10   # Layer 3: Code quality (PMD)
  metadata_diff: 0.15     # Layer 4: Configuration accuracy
  rubric: 0.15            # Layer 5: LLM-as-a-Judge

# PMD/Code Analyzer settings
pmd:
  enabled: true
  ruleset: default
  critical_weight: 3.0
  high_weight: 2.0
  medium_weight: 1.0
  low_weight: 0.5
  max_penalty: 0.10

# LLM-as-a-Judge rubric settings
rubric:
  enabled: true
  model: claude-3-5-sonnet-20241022
  temperature: 0.0
  max_tokens: 2048
  num_evaluations: 1

# Execution settings
parallel_runs: 1
timeout_minutes: 60
cleanup_orgs: true

# Logging
verbose: false
log_level: INFO
