# SF-AgentBench

**A Specialized Benchmarking Framework for Evaluating AI Agents on Salesforce Development**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

---

## Overview

SF-AgentBench is a rigorous benchmarking framework designed to evaluate AI agents‚Äîsuch as Claude Code, Codex, or Gemini Orchestrator‚Äîon their ability to design and build Salesforce solutions. While existing benchmarks like SWE-bench effectively assess code generation in file-based languages (Python, Java), they fail to capture the architectural complexity of Platform-as-a-Service (PaaS) environments like Salesforce.

Salesforce development is a hybrid practice requiring:
- **Declarative metadata** orchestration
- **Proprietary programming languages** (Apex, SOQL, LWC)
- **Stateful database interactions** within a multi-tenant environment
- **Strict execution limits** (Governor Limits)

SF-AgentBench addresses these unique challenges with a purpose-built evaluation framework.

## ‚ú® Features

### üñ•Ô∏è Interactive Terminal UI

A beautiful, user-friendly terminal interface built with [Textual](https://textual.textualize.io/):

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    üöÄ SF-AgentBench Dashboard                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ    5     ‚îÇ ‚îÇ    2     ‚îÇ ‚îÇ    2     ‚îÇ ‚îÇ    1     ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Total   ‚îÇ ‚îÇ  Tier 1  ‚îÇ ‚îÇ  Tier 2  ‚îÇ ‚îÇ  Tier 3  ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
‚îÇ  [Browse Tasks] [Run Benchmark] [View Results] [Configuration]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ D Dashboard ‚îÇ T Tasks ‚îÇ R Run ‚îÇ S Results ‚îÇ C Config ‚îÇ Q Quit   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**5 Interactive Screens:**
| Screen | Key | Description |
|--------|-----|-------------|
| Dashboard | `D` | Overview stats, quick actions, getting started |
| Tasks | `T` | Browse tasks by tier, view requirements |
| Run | `R` | Execute benchmarks with real-time progress |
| Results | `S` | Score history, layer breakdown, CSV export |
| Config | `C` | Edit all settings with tabbed interface |

### üéØ Curriculum-Aligned Evaluation

Grounded in official Salesforce certifications:
- **Administrator (ADM-201)** ‚Äî Schema, automation, security
- **Platform Developer I & II (PD1/PD2)** ‚Äî Apex, integrations, LWC

### üèÜ Superbadge Methodology

Uses complex, scenario-based problem solving as the gold standard‚Äîmoving beyond atomic code generation to holistic solution architecture.

### üîß Agent-Computer Interface (ACI)

11 tools wrapping the Salesforce CLI (`sf`) with structured JSON I/O:

| Tool | Description |
|------|-------------|
| `sf_deploy` | Deploy metadata to Scratch Org |
| `sf_retrieve` | Retrieve metadata from org |
| `sf_run_apex_tests` | Execute Apex tests with coverage |
| `sf_run_anonymous` | Run anonymous Apex code |
| `sf_query` | Execute SOQL queries |
| `sf_create_record` | Create sObject records |
| `sf_import_data` | Import data from plan files |
| `sf_scan_code` | Run PMD static analysis |
| `sf_org_create` | Create Scratch Orgs |
| `sf_org_delete` | Delete Scratch Orgs |
| `sf_org_open` | Get org login URL |

### üìä 5-Layer Evaluation Pipeline

| Layer | Weight | Metric | Description |
|-------|--------|--------|-------------|
| **1** | 20% | Deployment | Can the solution deploy without errors? |
| **2** | 40% | Functional Tests | Do Apex tests pass? What's the coverage? |
| **3** | 10% | Static Analysis | Code quality via PMD/Code Analyzer |
| **4** | 15% | Metadata Diff | Semantic comparison against golden config |
| **5** | 15% | LLM Rubric | Design patterns, bulkification, best practices |

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     SF-AgentBench Harness                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Task      ‚îÇ  ‚îÇ   Agent     ‚îÇ  ‚îÇ      Evaluation         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Loader    ‚îÇ  ‚îÇ   Runner    ‚îÇ  ‚îÇ      Pipeline           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                    Agent-Computer Interface (ACI)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇsf_deploy ‚îÇ ‚îÇsf_query  ‚îÇ ‚îÇsf_test   ‚îÇ ‚îÇsf_scan_code      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                     Salesforce CLI (sf)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                   Ephemeral Scratch Org Pool                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Project Structure

```
sf-agentbench/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml              # Python package configuration
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ sf-agentbench.yaml          # Main configuration file
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ development/
‚îÇ       ‚îú‚îÄ‚îÄ Salesforce AI Benchmark Design.md
‚îÇ       ‚îî‚îÄ‚îÄ Salesforce AI Benchmark Design.pdf
‚îú‚îÄ‚îÄ src/sf_agentbench/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                  # CLI entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ models.py               # Data models (Task, Result, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ aci/                    # Agent-Computer Interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # Base tool class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deploy.py           # sf_deploy, sf_retrieve
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ apex.py             # sf_run_apex_tests, sf_run_anonymous
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.py             # sf_query, sf_create_record, sf_import_data
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py         # sf_scan_code
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ org.py              # Scratch org management
‚îÇ   ‚îú‚îÄ‚îÄ harness/                # Benchmark orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ runner.py           # BenchmarkHarness
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task_loader.py      # Task discovery
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ org_manager.py      # Scratch Org lifecycle
‚îÇ   ‚îú‚îÄ‚îÄ evaluators/             # 5-layer evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py         # Main pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.py       # Layer 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ functional.py       # Layer 2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ static_analysis.py  # Layer 3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metadata_diff.py    # Layer 4
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rubric.py           # Layer 5
‚îÇ   ‚îú‚îÄ‚îÄ storage/                # Results persistence
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ store.py            # SQLite-based ResultsStore
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py           # RunRecord, RunSummary, AgentComparison
‚îÇ   ‚îî‚îÄ‚îÄ tui/                    # Terminal User Interface
‚îÇ       ‚îú‚îÄ‚îÄ app.py              # Main TUI application
‚îÇ       ‚îî‚îÄ‚îÄ screens/            # Dashboard, Tasks, Run, Results, Config
‚îú‚îÄ‚îÄ tasks/                      # Benchmark tasks
‚îÇ   ‚îú‚îÄ‚îÄ tier-1/
‚îÇ   ‚îî‚îÄ‚îÄ tier-2/
‚îú‚îÄ‚îÄ results/                    # Run outputs & database
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_results.db    # SQLite database with all runs
‚îÇ   ‚îú‚îÄ‚îÄ runs/                   # Per-run detailed JSON files
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_results.json  # Exported summary JSON
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_results.csv   # Exported CSV (optional)
‚îî‚îÄ‚îÄ tests/                      # Test suite
```

## üöÄ Getting Started

### Prerequisites

- **Python 3.10+**
- **Salesforce CLI** (`sf`) ‚Äî [Install Guide](https://developer.salesforce.com/tools/salesforcecli)
- **DevHub-enabled Org** ‚Äî Required for Scratch Org creation

### Installation

```bash
# Clone the repository
git clone https://github.com/bhanudas/sf-agentbench.git
cd sf-agentbench

# Install in development mode
pip install -e .

# Initialize with sample tasks
sf-agentbench init
```

### Quick Start

#### Launch the Terminal UI (Recommended)

```bash
sf-agentbench-tui
```

Navigate with keyboard shortcuts:
- `D` - Dashboard
- `T` - Browse Tasks
- `R` - Run Benchmark
- `S` - View Results
- `C` - Configuration
- `Q` - Quit

#### Use the CLI

```bash
# List available tasks
sf-agentbench list-tasks

# Show task details
sf-agentbench show-task lead-scoring-validation

# Run a specific task
sf-agentbench run lead-scoring-validation --agent claude-code

# Validate your setup
sf-agentbench validate
```

## üíæ Results Storage

SF-AgentBench uses a robust storage system for benchmark results:

### SQLite Database

All runs are persisted to a SQLite database (`results/benchmark_results.db`) with:
- **Run metadata**: task, agent, timestamps, status
- **Layer scores**: deployment, tests, static analysis, metadata, rubric
- **Final composite score**

### Per-Run Detail Files

Each run creates a directory (`results/runs/{run_id}/`) containing:
- `result.json` ‚Äî Full evaluation details
- `agent_output.txt` ‚Äî Agent's raw output

### Querying Results

```python
from sf_agentbench.storage import ResultsStore
from pathlib import Path

store = ResultsStore(Path("results"))

# Get summary statistics
summary = store.get_summary()
print(f"Total runs: {summary.total_runs}")
print(f"Average score: {summary.average_score:.2f}")

# List recent runs
runs = store.list_runs(limit=10)
for run in runs:
    print(f"{run.run_id}: {run.task_id} -> {run.final_score:.2f}")

# Compare agents
comparisons = store.get_agent_comparison()
for agent in comparisons:
    print(f"{agent.agent_id}: {agent.average_score:.2f} avg")

# Export to CSV
store.export_to_csv(Path("results/export.csv"))
```

### Export Formats

- **JSON**: `sf-agentbench export --format json`
- **CSV**: `sf-agentbench export --format csv` or use the TUI export button

## ‚öôÔ∏è Configuration

Edit `sf-agentbench.yaml` to configure:

```yaml
# Agent configuration
agent:
  id: claude-code
  type: claude
  model: claude-sonnet-4-20250514
  api_key_env: ANTHROPIC_API_KEY

# Salesforce settings
devhub_username: admin@mydevhub.org

# Evaluation weights (must sum to 1.0)
evaluation_weights:
  deployment: 0.20
  functional_tests: 0.40
  static_analysis: 0.10
  metadata_diff: 0.15
  rubric: 0.15
```

## üìã Task Difficulty Tiers

| Tier | Complexity | Example | Skills Tested |
|------|------------|---------|---------------|
| **Tier 1** | Single-domain, declarative | Validation Rule + Flow | Schema, Validation Rules, Flows |
| **Tier 2** | Multi-domain, declarative + code | Screen Flow + Apex Action | Screen Flow, Invocable Apex, Testing |
| **Tier 3** | Complex code, async processing | Apex Specialist Superbadge | Triggers, Queueable, Bulkification |
| **Tier 4** | Full-stack, LWC, integrations | LWC Specialist Superbadge | LWC, Apex Services, Wire, Callouts |

## üìà Scoring Methodology

The composite score combines all evaluation layers:

```
Final_Score = (
    0.20 √ó deployment_success +
    0.40 √ó apex_test_pass_rate +
    0.10 √ó (1 - pmd_penalty) +
    0.15 √ó metadata_accuracy +
    0.15 √ó rubric_score
)
```

Score indicators:
- üü¢ **Excellent**: ‚â• 0.80
- üü° **Good**: ‚â• 0.60
- üî¥ **Needs Work**: < 0.60

## üó∫Ô∏è Roadmap

### Phase 1: Foundation ‚úÖ
- [x] ACI tool wrappers for core `sf` commands
- [x] Basic harness for task loading and evaluation
- [x] 5-layer evaluation pipeline
- [x] Terminal UI with Textual
- [x] Sample Tier 1 & 2 tasks

### Phase 2: Expansion
- [ ] DevHub setup with Scratch Org pool management
- [ ] PMD/Code Analyzer deep integration
- [ ] 10 Tier 3 tasks
- [ ] Baseline runs with leading AI agents

### Phase 3: Maturity
- [ ] LLM-as-a-Judge with multiple providers
- [ ] 5 Tier 4 tasks
- [ ] Public leaderboard
- [ ] Research paper submission

## üìö Documentation

- [Technical Design Document](docs/development/Salesforce%20AI%20Benchmark%20Design.md) ‚Äî Comprehensive framework architecture and methodology

## ü§ù Contributing

Contributions are welcome! Areas for contribution:
- New benchmark tasks (especially Tier 3 & 4)
- ACI tool enhancements
- Evaluation metric refinements
- Documentation improvements

## üìÑ License

This project is licensed under the MIT License ‚Äî see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Inspired by [SWE-bench](https://www.swebench.com/) and [SWE-agent](https://swe-agent.com/)
- Task methodology adapted from [Salesforce Trailhead Superbadges](https://trailhead.salesforce.com/superbadges)
- Built for the AI agent research community

---

<p align="center">
  <strong>SF-AgentBench</strong> ‚Äî Bridging AI Agents and Enterprise Platform Development
</p>
